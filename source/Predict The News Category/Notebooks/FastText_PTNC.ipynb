{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FastText_PTNC.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XKY6eiwrqai"
      },
      "source": [
        "\n",
        "# Predict The News Category : Anshul Patel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbumugyYr2H0"
      },
      "source": [
        "#### Import General Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfzMYENIZFdM",
        "outputId": "d442768c-3ce2-4e65-efb3-a01cdf46c0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install ktrain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ktrain\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b4/a614efba8fe41ba3ba8ea9300aeceac4bdb3cdb41c4796b4515f490aa1cc/ktrain-0.23.2.tar.gz (25.3MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3MB 128kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.2.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.1.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.0.0)\n",
            "Collecting keras_bert>=0.86.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.17.0)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.42.1)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/e5/a0b9edd8664ea3b0d3270c451ebbf86655ed9fc4c3e4c45b9afae9c2e382/cchardet-2.1.7-cp36-cp36m-manylinux2010_x86_64.whl (263kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.5)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.1.1)\n",
            "Collecting seqeval==0.0.19\n",
            "  Downloading https://files.pythonhosted.org/packages/93/e5/b7705156a77f742cfe4fc6f22d0c71591edb2d243328dff2f8fc0f933ab6/seqeval-0.0.19.tar.gz\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.4)\n",
            "Collecting transformers>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 54.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain) (5.5.0)\n",
            "Collecting syntok\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/76/a49e73a04b3e3a14ce232e8e28a1587f8108baa665644fe8c40e307e792e/syntok-1.3.1.tar.gz\n",
            "Collecting whoosh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/19/24d0f1f454a2c1eb689ca28d2f178db81e5024f42d82729a4ff6771155cf/Whoosh-2.7.4-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 30.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->ktrain) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->ktrain) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras_bert>=0.86.0->ktrain) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->ktrain) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.3->ktrain) (4.4.2)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (5.1.1)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.11.2)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (7.0.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.7.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (3.12.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.1.0->ktrain) (3.0.12)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 53.7MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 56.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (50.3.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras_bert>=0.86.0->ktrain) (2.10.0)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.1.0->ktrain) (7.1.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: ktrain, keras-bert, langdetect, seqeval, syntok, keras-transformer, sacremoses, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.23.2-cp36-none-any.whl size=25272006 sha256=443cfe83b30f0d8569d41f4529161acb3200a435cbd9b6e13f5e2dfa977a136b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/4a/b1/aff404c4e0893ca1c0b64781d0298b7ff6de94d5117a0e7d5c\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=5d8a7070be4e76e1ac598d23589547dca45922798778bb09bc8ab53087c71258\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=2fc18dc5cb59111f0ef9c1876f3a5417ac5b6971eab5b2b03cb1269e3acc3810\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.19-cp36-none-any.whl size=9919 sha256=fa8316d0be10127116f41fd6dc6ee705847ae9cfa1ab8c95cda07bef634e040e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/bf/1198beceed805a2099060975f6281d1b01046dd279e19c97be\n",
            "  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for syntok: filename=syntok-1.3.1-cp36-none-any.whl size=20919 sha256=7801994c1676fc67e553ef9e4f430b3b34ebde071894040dbcb09fe709191e9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/c6/a4/be1920586c49469846bcd2888200bdecfe109ec421dab9be2d\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=1132bd018ab0901aa158a6fa68a6c1a9fcb01b125d1982c82eec7cc40f8831c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=fca0698733baeb6b4bf512271a72d5907940a660446b1f3dcde87de1a5ff926b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=f590f81906b4ad4b07fcf9512bc4d679ea31bc12ff323b1fabdc70492a00fe0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=cd01c12467e0d175f22d134f4e64a8ca015e682a67932b2e72df81cd6fc021b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=3c7f28ca2eaa3810c64c1d16eb352e3aacc64ae509a583b377c4e64e9e671a2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=897bf362446767fcb3da3279af9afe5e1d480303e6708dc6d1c74cd52c2bb88b\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=d5b5d665c2483c42563accb3eec764c56c97c68a7349f616e358bfbc966b2fe6\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=6a10187a533705bedfeb99313207a462bf6610ef6d19fed0683af71218227b48\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built ktrain keras-bert langdetect seqeval syntok keras-transformer sacremoses keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, langdetect, cchardet, seqeval, sacremoses, sentencepiece, tokenizers, transformers, syntok, whoosh, ktrain\n",
            "Successfully installed cchardet-2.1.7 keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0 ktrain-0.23.2 langdetect-1.0.8 sacremoses-0.0.43 sentencepiece-0.1.94 seqeval-0.0.19 syntok-1.3.1 tokenizers-0.9.2 transformers-3.4.0 whoosh-2.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l--10Pr-ZL4l"
      },
      "source": [
        "import ktrain\n",
        "from ktrain import text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW4s4FIVsFfQ"
      },
      "source": [
        "#### Import Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyWINi5AZL15"
      },
      "source": [
        "DATA_PATH = '/content/drive/My Drive/MH/PTNC/Data_Train.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkTtDlwbsRPR"
      },
      "source": [
        "#### Preprocessing Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtmXH11qZLzP",
        "outputId": "c35e5982-3dcb-47e2-b0da-b6f0943a3ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "MAXLEN = 500\n",
        "NUM_WORDS = 50000\n",
        "(x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n",
        "                      'Text',\n",
        "                      label_columns = [\"Label\"],\n",
        "                      val_filepath=None,\n",
        "                      max_features=NUM_WORDS,\n",
        "                      maxlen=MAXLEN,\n",
        "                      ngram_range=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "detected encoding: WINDOWS-1252 (if wrong, set manually)\n",
            "language: en\n",
            "Word Counts: 35997\n",
            "Nrows: 6865\n",
            "6865 train sequences\n",
            "train sequence lengths:\n",
            "\tmean : 110\n",
            "\t95percentile : 257\n",
            "\t99percentile : 465\n",
            "x_train shape: (6865,500)\n",
            "y_train shape: (6865, 4)\n",
            "Is Multi-Label? False\n",
            "763 test sequences\n",
            "test sequence lengths:\n",
            "\tmean : 109\n",
            "\t95percentile : 232\n",
            "\t99percentile : 483\n",
            "x_test shape: (763,500)\n",
            "y_test shape: (763, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVpybZx4sibA"
      },
      "source": [
        "#### Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWBnxzEgZLwk",
        "outputId": "2fca81f2-1066-49f6-ef00-85f39c15a2c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "model = text.text_classifier('fasttext', (x_train, y_train), \n",
        "                             preproc=preproc)\n",
        "learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "compiling word ID features...\n",
            "maxlen is 500\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQj4hsKEZkhD",
        "outputId": "9304bdf2-73f2-4071-b01c-9bffa2c6adc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "learner.lr_find(max_epochs=5)\n",
        "learner.lr_plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "simulating training for different learning rates... this may take a few moments...\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 2.0459 - accuracy: 0.2242\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 2.0477 - accuracy: 0.2189\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 1.3235 - accuracy: 0.4503\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.8557 - accuracy: 0.6839\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 1.5328 - accuracy: 0.3098\n",
            "\n",
            "\n",
            "done.\n",
            "Please invoke the Learner.lr_plot() method to visually inspect the loss plot to help identify the maximal learning rate associated with falling loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSW2qYdUssPb"
      },
      "source": [
        "#### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7KrpTfpZkgi",
        "outputId": "d266a1c0-a15c-4b7f-cc2c-18aeff7c810c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "learner.autofit(0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "early_stopping automatically enabled at patience=5\n",
            "reduce_on_plateau automatically enabled at patience=2\n",
            "\n",
            "\n",
            "begin training using triangular learning rate policy with max lr of 0.001...\n",
            "Epoch 1/1024\n",
            "215/215 [==============================] - 4s 16ms/step - loss: 1.5191 - accuracy: 0.3710 - val_loss: 1.2748 - val_accuracy: 0.4535\n",
            "Epoch 2/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.8986 - accuracy: 0.6358 - val_loss: 0.7723 - val_accuracy: 0.8571\n",
            "Epoch 3/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.5846 - accuracy: 0.7828 - val_loss: 0.2773 - val_accuracy: 0.9397\n",
            "Epoch 4/1024\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.4396 - accuracy: 0.8468 - val_loss: 0.1811 - val_accuracy: 0.9515\n",
            "Epoch 5/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.3286 - accuracy: 0.8874 - val_loss: 0.1491 - val_accuracy: 0.9594\n",
            "Epoch 6/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.2829 - accuracy: 0.9136 - val_loss: 0.1366 - val_accuracy: 0.9567\n",
            "Epoch 7/1024\n",
            "215/215 [==============================] - 4s 16ms/step - loss: 0.2331 - accuracy: 0.9237 - val_loss: 0.1227 - val_accuracy: 0.9607\n",
            "Epoch 8/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.2092 - accuracy: 0.9296 - val_loss: 0.1127 - val_accuracy: 0.9594\n",
            "Epoch 9/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.1953 - accuracy: 0.9374 - val_loss: 0.1043 - val_accuracy: 0.9685\n",
            "Epoch 10/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.1646 - accuracy: 0.9467 - val_loss: 0.1019 - val_accuracy: 0.9712\n",
            "Epoch 11/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.1460 - accuracy: 0.9546 - val_loss: 0.1013 - val_accuracy: 0.9685\n",
            "Epoch 12/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.1376 - accuracy: 0.9551 - val_loss: 0.1008 - val_accuracy: 0.9699\n",
            "Epoch 13/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.1269 - accuracy: 0.9573 - val_loss: 0.1011 - val_accuracy: 0.9738\n",
            "Epoch 14/1024\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9597\n",
            "Epoch 00014: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.1145 - accuracy: 0.9598 - val_loss: 0.1034 - val_accuracy: 0.9725\n",
            "Epoch 15/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.1060 - accuracy: 0.9678 - val_loss: 0.0996 - val_accuracy: 0.9751\n",
            "Epoch 16/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0960 - accuracy: 0.9672 - val_loss: 0.0990 - val_accuracy: 0.9751\n",
            "Epoch 17/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0893 - accuracy: 0.9701 - val_loss: 0.0985 - val_accuracy: 0.9738\n",
            "Epoch 18/1024\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.0983 - accuracy: 0.9684 - val_loss: 0.0941 - val_accuracy: 0.9738\n",
            "Epoch 19/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0949 - accuracy: 0.9717 - val_loss: 0.0936 - val_accuracy: 0.9738\n",
            "Epoch 20/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0862 - accuracy: 0.9722 - val_loss: 0.0939 - val_accuracy: 0.9777\n",
            "Epoch 21/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0962 - accuracy: 0.9684 - val_loss: 0.0933 - val_accuracy: 0.9764\n",
            "Epoch 22/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0779 - accuracy: 0.9768 - val_loss: 0.0979 - val_accuracy: 0.9751\n",
            "Epoch 23/1024\n",
            "212/215 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9757\n",
            "Epoch 00023: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.0719 - accuracy: 0.9758 - val_loss: 0.0963 - val_accuracy: 0.9751\n",
            "Epoch 24/1024\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0740 - accuracy: 0.9776 - val_loss: 0.0964 - val_accuracy: 0.9738\n",
            "Epoch 25/1024\n",
            "213/215 [============================>.] - ETA: 0s - loss: 0.0672 - accuracy: 0.9789\n",
            "Epoch 00025: Reducing Max LR on Plateau: new max lr will be 0.000125 (if not early_stopping).\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0676 - accuracy: 0.9787 - val_loss: 0.0962 - val_accuracy: 0.9738\n",
            "Epoch 26/1024\n",
            "214/215 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9763Restoring model weights from the end of the best epoch.\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.0718 - accuracy: 0.9764 - val_loss: 0.0943 - val_accuracy: 0.9751\n",
            "Epoch 00026: early stopping\n",
            "Weights from best epoch have been loaded into model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd46bf35b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLk3rDPgs31d"
      },
      "source": [
        "#### Predict on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw4U1F58ZvTW"
      },
      "source": [
        "predictor = ktrain.get_predictor(learner.model, preproc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLO-I4ecbygY",
        "outputId": "d8553e92-6ea3-4b03-ccb9-f02853ed5d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "import pandas as pd\n",
        "docs = pd.read_csv('/content/drive/My Drive/MH/PTNC/Data_Test.csv',encoding='WINDOWS-1252')\n",
        "docs.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019 will see gadgets like gaming smartphones ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It has also unleashed a wave of changes in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It can be confusing to pick the right smartpho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The mobile application is integrated with a da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We have rounded up some of the gadgets that sh...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                TEXT\n",
              "0  2019 will see gadgets like gaming smartphones ...\n",
              "1  It has also unleashed a wave of changes in the...\n",
              "2  It can be confusing to pick the right smartpho...\n",
              "3  The mobile application is integrated with a da...\n",
              "4  We have rounded up some of the gadgets that sh..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC5LyNE5c1o5"
      },
      "source": [
        "data = docs['TEXT'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR7ygqWzbyvj",
        "outputId": "b25866c4-3787-4786-8be7-c36dc46661cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_result = predictor.predict(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_0',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_2',\n",
              " 'Label_0',\n",
              " 'Label_3',\n",
              " 'Label_1',\n",
              " 'Label_3',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " 'Label_1',\n",
              " 'Label_2',\n",
              " 'Label_1',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9pcfzbpc9zG"
      },
      "source": [
        "for i in range(len(test_result)):\n",
        "  test_result[i] = int(test_result[i][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcfNJFMKc_Wt"
      },
      "source": [
        "df_T = pd.DataFrame(test_result)\n",
        "df_T.to_excel(excel_writer = \"/content/drive/My Drive/MH/PTNC/FastTextResult.xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWxdtjLjtEK7"
      },
      "source": [
        "# RESULT ON PUBLIC DATA : 0.97197"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_2TYZC2dae3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}